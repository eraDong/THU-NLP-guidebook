{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# å®Œæ•´ä»£ç åˆ†ä¸ºäº”ä¸ªéƒ¨åˆ†\n",
    "\n",
    "æœ¬æ¬¡ä»»åŠ¡çš„ç›®æ ‡æ•°æ®é›†æ˜¯QNLIï¼Œå®Œæˆè‡ªç„¶è¯­è¨€æ¨æ–­ä»»åŠ¡ã€‚\n",
    "\n",
    "ç»™å®šä¸€ä¸ªé—®å¥ï¼Œéœ€è¦åˆ¤æ–­ç»™å®šæ–‡æœ¬ä¸­æ˜¯å¦åŒ…å«è¯¥é—®å¥çš„æ­£ç¡®ç­”æ¡ˆã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ä¸‹è½½æ•°æ®é›†"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "åˆ©ç”¨load_datasetä¸‹è½½æ•°æ®é›†\n",
    "\n",
    "åˆ©ç”¨load_metricä¸‹è½½æ•°æ®é›†è¯„æµ‹æŒ‡æ ‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\11844\\AppData\\Local\\Temp\\ipykernel_10324\\4197946858.py:5: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ğŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric(\"glue\", \"qnli\")\n",
      "e:\\anaconda\\envs\\d2l\\lib\\site-packages\\datasets\\load.py:752: FutureWarning: The repository for glue contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.16.1/metrics/glue/glue.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, load_metric\n",
    "\n",
    "dataset = load_dataset(\"glue\", \"qnli\")\n",
    "\n",
    "metric = load_metric(\"glue\", \"qnli\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ç®€å•æŸ¥çœ‹ä¸€ä¸‹æ•°æ®é›†ä»¥åŠè¯„æµ‹æŒ‡æ ‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['question', 'sentence', 'label', 'idx'],\n",
      "        num_rows: 104743\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['question', 'sentence', 'label', 'idx'],\n",
      "        num_rows: 5463\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['question', 'sentence', 'label', 'idx'],\n",
      "        num_rows: 5463\n",
      "    })\n",
      "}) {'question': 'When did the third Digimon series begin?', 'sentence': 'Unlike the two seasons before it and most of the seasons that followed, Digimon Tamers takes a darker and more realistic approach to its story featuring Digimon who do not reincarnate after their deaths and more complex character development in the original Japanese.', 'label': 1, 'idx': 0}\n"
     ]
    }
   ],
   "source": [
    "print(dataset,dataset['train'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric(name: \"glue\", features: {'predictions': Value(dtype='int64', id=None), 'references': Value(dtype='int64', id=None)}, usage: \"\"\"\n",
      "Compute GLUE evaluation metric associated to each GLUE dataset.\n",
      "Args:\n",
      "    predictions: list of predictions to score.\n",
      "        Each translation should be tokenized into a list of tokens.\n",
      "    references: list of lists of references for each translation.\n",
      "        Each reference should be tokenized into a list of tokens.\n",
      "Returns: depending on the GLUE subset, one or several of:\n",
      "    \"accuracy\": Accuracy\n",
      "    \"f1\": F1 score\n",
      "    \"pearson\": Pearson Correlation\n",
      "    \"spearmanr\": Spearman Correlation\n",
      "    \"matthews_correlation\": Matthew Correlation\n",
      "Examples:\n",
      "\n",
      "    >>> glue_metric = datasets.load_metric('glue', 'sst2')  # 'sst2' or any of [\"mnli\", \"mnli_mismatched\", \"mnli_matched\", \"qnli\", \"rte\", \"wnli\", \"hans\"]\n",
      "    >>> references = [0, 1]\n",
      "    >>> predictions = [0, 1]\n",
      "    >>> results = glue_metric.compute(predictions=predictions, references=references)\n",
      "    >>> print(results)\n",
      "    {'accuracy': 1.0}\n",
      "\n",
      "    >>> glue_metric = datasets.load_metric('glue', 'mrpc')  # 'mrpc' or 'qqp'\n",
      "    >>> references = [0, 1]\n",
      "    >>> predictions = [0, 1]\n",
      "    >>> results = glue_metric.compute(predictions=predictions, references=references)\n",
      "    >>> print(results)\n",
      "    {'accuracy': 1.0, 'f1': 1.0}\n",
      "\n",
      "    >>> glue_metric = datasets.load_metric('glue', 'stsb')\n",
      "    >>> references = [0., 1., 2., 3., 4., 5.]\n",
      "    >>> predictions = [0., 1., 2., 3., 4., 5.]\n",
      "    >>> results = glue_metric.compute(predictions=predictions, references=references)\n",
      "    >>> print({\"pearson\": round(results[\"pearson\"], 2), \"spearmanr\": round(results[\"spearmanr\"], 2)})\n",
      "    {'pearson': 1.0, 'spearmanr': 1.0}\n",
      "\n",
      "    >>> glue_metric = datasets.load_metric('glue', 'cola')\n",
      "    >>> references = [0, 1]\n",
      "    >>> predictions = [0, 1]\n",
      "    >>> results = glue_metric.compute(predictions=predictions, references=references)\n",
      "    >>> print(results)\n",
      "    {'matthews_correlation': 1.0}\n",
      "\"\"\", stored examples: 0)\n"
     ]
    }
   ],
   "source": [
    "print(metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## å¯¹æ•°æ®é›†è¿›è¡Œtokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/bert_uncased_L-2_H-128_A-2\")\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples['sentence'], truncation=True, max_length=512)\n",
    "\n",
    "encoded_data = dataset.map(preprocess_function,batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "è§‚å¯Ÿä¸€ä¸‹æ˜¯å¦tokenizationæˆåŠŸ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': ['When did the third Digimon series begin?', 'Which missile batteries often have individual launchers several kilometres from one another?', \"What two things does Popper argue Tarski's theory involves in an evaluation of truth?\", 'What is the name of the village 9 miles north of Calafat where the Ottoman forces attacked the Russians?', 'What famous palace is located in London?'], 'sentence': ['Unlike the two seasons before it and most of the seasons that followed, Digimon Tamers takes a darker and more realistic approach to its story featuring Digimon who do not reincarnate after their deaths and more complex character development in the original Japanese.', 'When MANPADS is operated by specialists, batteries may have several dozen teams deploying separately in small sections; self-propelled air defence guns may deploy in pairs.', 'He bases this interpretation on the fact that examples such as the one described above refer to two things: assertions and the facts to which they refer.', 'On 31 December 1853, the Ottoman forces at Calafat moved against the Russian force at Chetatea or Cetate, a small village nine miles north of Calafat, and engaged them on 6 January 1854.', \"London contains four World Heritage Sites: the Tower of London; Kew Gardens; the site comprising the Palace of Westminster, Westminster Abbey, and St Margaret's Church; and the historic settlement of Greenwich (in which the Royal Observatory, Greenwich marks the Prime Meridian, 0Â° longitude, and GMT).\"], 'label': [1, 1, 0, 0, 1], 'idx': [0, 1, 2, 3, 4], 'input_ids': [[101, 4406, 1996, 2048, 3692, 2077, 2009, 1998, 2087, 1997, 1996, 3692, 2008, 2628, 1010, 10667, 16339, 2078, 24763, 2869, 3138, 1037, 9904, 1998, 2062, 12689, 3921, 2000, 2049, 2466, 3794, 10667, 16339, 2078, 2040, 2079, 2025, 27788, 10010, 12556, 2044, 2037, 6677, 1998, 2062, 3375, 2839, 2458, 1999, 1996, 2434, 2887, 1012, 102], [101, 2043, 2158, 15455, 2015, 2003, 3498, 2011, 15744, 1010, 10274, 2089, 2031, 2195, 6474, 2780, 21296, 2075, 10329, 1999, 2235, 5433, 1025, 2969, 1011, 15801, 2250, 4721, 4409, 2089, 21296, 1999, 7689, 1012, 102], [101, 2002, 7888, 2023, 7613, 2006, 1996, 2755, 2008, 4973, 2107, 2004, 1996, 2028, 2649, 2682, 6523, 2000, 2048, 2477, 1024, 23617, 2015, 1998, 1996, 8866, 2000, 2029, 2027, 6523, 1012, 102], [101, 2006, 2861, 2285, 8933, 1010, 1996, 6188, 2749, 2012, 10250, 10354, 4017, 2333, 2114, 1996, 2845, 2486, 2012, 25157, 3686, 2050, 2030, 8292, 12259, 1010, 1037, 2235, 2352, 3157, 2661, 2167, 1997, 10250, 10354, 4017, 1010, 1998, 5117, 2068, 2006, 1020, 2254, 8421, 1012, 102], [101, 2414, 3397, 2176, 2088, 4348, 4573, 1024, 1996, 3578, 1997, 2414, 1025, 17710, 2860, 5822, 1025, 1996, 2609, 9605, 1996, 4186, 1997, 9434, 1010, 9434, 6103, 1010, 1998, 2358, 5545, 1005, 1055, 2277, 1025, 1998, 1996, 3181, 4093, 1997, 13861, 1006, 1999, 2029, 1996, 2548, 9970, 1010, 13861, 6017, 1996, 3539, 17984, 1010, 1014, 7737, 20413, 1010, 1998, 13938, 2102, 1007, 1012, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n"
     ]
    }
   ],
   "source": [
    "print(encoded_data['train'][:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æ¨¡å‹ä¸‹è½½"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\"google/bert_uncased_L-2_H-128_A-2\",num_labels=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# è®¾å®šå‚æ•°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"./result\",\n",
    "    evaluation_strategy=\"epoch\", # åœ¨æ¯ä¸ªepochç»“æŸåæµ‹è¯•æ•ˆæœ\n",
    "    save_strategy=\"epoch\", # æ¯ä¸ªepochç»“æŸåä¿å­˜æ¨¡å‹\n",
    "    learning_rate=2e-5, # å­¦ä¹ ç‡\n",
    "    per_device_train_batch_size=batch_size, # æ¯ä¸ªGPUè®­ç»ƒçš„batch size\n",
    "    per_device_eval_batch_size=batch_size, # æ¯ä¸ªGPUæµ‹è¯•çš„batch size\n",
    "    num_train_epochs=5, # è®­ç»ƒçš„epochæ•°\n",
    "    weight_decay=0.01, # æƒé‡è¡°å‡\n",
    "    load_best_model_at_end=True, # è®­ç»ƒç»“æŸåï¼Œæ˜¯å¦åŠ è½½åœ¨éªŒè¯é›†ä¸Šè¡¨ç°æœ€å¥½çš„æ¨¡å‹\n",
    "    metric_for_best_model=\"accuracy\" # å‡†ç¡®ç‡ä¸ºæŒ‡æ ‡\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## è®­ç»ƒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, lables = eval_pred\n",
    "    predictions = np.argmax(logits,axis=1)\n",
    "    return metric.compute(predictions=predictions, references=lables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=encoded_data[\"train\"],\n",
    "    eval_dataset=encoded_data[\"validation\"],\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22ec74055e3b4bff9e5213807e451f83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32735 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6887, 'learning_rate': 1.9694516572475945e-05, 'epoch': 0.08}\n",
      "{'loss': 0.6715, 'learning_rate': 1.9389033144951888e-05, 'epoch': 0.15}\n",
      "{'loss': 0.6656, 'learning_rate': 1.908354971742783e-05, 'epoch': 0.23}\n",
      "{'loss': 0.6595, 'learning_rate': 1.8778066289903775e-05, 'epoch': 0.31}\n",
      "{'loss': 0.6584, 'learning_rate': 1.8472582862379718e-05, 'epoch': 0.38}\n",
      "{'loss': 0.6561, 'learning_rate': 1.816709943485566e-05, 'epoch': 0.46}\n",
      "{'loss': 0.6567, 'learning_rate': 1.7861616007331604e-05, 'epoch': 0.53}\n",
      "{'loss': 0.657, 'learning_rate': 1.7556132579807548e-05, 'epoch': 0.61}\n",
      "{'loss': 0.6568, 'learning_rate': 1.725064915228349e-05, 'epoch': 0.69}\n",
      "{'loss': 0.6604, 'learning_rate': 1.6945165724759434e-05, 'epoch': 0.76}\n",
      "{'loss': 0.6575, 'learning_rate': 1.6639682297235374e-05, 'epoch': 0.84}\n",
      "{'loss': 0.6457, 'learning_rate': 1.633419886971132e-05, 'epoch': 0.92}\n",
      "{'loss': 0.6518, 'learning_rate': 1.6028715442187264e-05, 'epoch': 0.99}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17f1c3a394c84bf2af417f5472476796",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/342 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6497064232826233, 'eval_accuracy': 0.6121178839465495, 'eval_runtime': 8.0978, 'eval_samples_per_second': 674.626, 'eval_steps_per_second': 42.234, 'epoch': 1.0}\n",
      "{'loss': 0.6447, 'learning_rate': 1.5723232014663207e-05, 'epoch': 1.07}\n",
      "{'loss': 0.6458, 'learning_rate': 1.541774858713915e-05, 'epoch': 1.15}\n",
      "{'loss': 0.6482, 'learning_rate': 1.5112265159615092e-05, 'epoch': 1.22}\n",
      "{'loss': 0.6424, 'learning_rate': 1.4806781732091035e-05, 'epoch': 1.3}\n",
      "{'loss': 0.6453, 'learning_rate': 1.4501298304566977e-05, 'epoch': 1.37}\n",
      "{'loss': 0.6414, 'learning_rate': 1.4195814877042922e-05, 'epoch': 1.45}\n",
      "{'loss': 0.6532, 'learning_rate': 1.3890331449518865e-05, 'epoch': 1.53}\n",
      "{'loss': 0.6424, 'learning_rate': 1.3584848021994808e-05, 'epoch': 1.6}\n",
      "{'loss': 0.647, 'learning_rate': 1.3279364594470752e-05, 'epoch': 1.68}\n",
      "{'loss': 0.6456, 'learning_rate': 1.2973881166946693e-05, 'epoch': 1.76}\n",
      "{'loss': 0.6464, 'learning_rate': 1.2668397739422638e-05, 'epoch': 1.83}\n",
      "{'loss': 0.6456, 'learning_rate': 1.236291431189858e-05, 'epoch': 1.91}\n",
      "{'loss': 0.6485, 'learning_rate': 1.2057430884374523e-05, 'epoch': 1.99}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a24d9a92c236429fa1ee7592a3c99c79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/342 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.644970715045929, 'eval_accuracy': 0.613399231191653, 'eval_runtime': 8.3942, 'eval_samples_per_second': 650.81, 'eval_steps_per_second': 40.743, 'epoch': 2.0}\n",
      "{'loss': 0.6396, 'learning_rate': 1.1751947456850468e-05, 'epoch': 2.06}\n",
      "{'loss': 0.6418, 'learning_rate': 1.144646402932641e-05, 'epoch': 2.14}\n",
      "{'loss': 0.6388, 'learning_rate': 1.1140980601802354e-05, 'epoch': 2.21}\n",
      "{'loss': 0.6322, 'learning_rate': 1.0835497174278296e-05, 'epoch': 2.29}\n",
      "{'loss': 0.6389, 'learning_rate': 1.053001374675424e-05, 'epoch': 2.37}\n",
      "{'loss': 0.6368, 'learning_rate': 1.0224530319230184e-05, 'epoch': 2.44}\n",
      "{'loss': 0.6402, 'learning_rate': 9.919046891706126e-06, 'epoch': 2.52}\n",
      "{'loss': 0.6421, 'learning_rate': 9.613563464182069e-06, 'epoch': 2.6}\n",
      "{'loss': 0.6384, 'learning_rate': 9.308080036658012e-06, 'epoch': 2.67}\n",
      "{'loss': 0.6372, 'learning_rate': 9.002596609133956e-06, 'epoch': 2.75}\n",
      "{'loss': 0.6363, 'learning_rate': 8.697113181609899e-06, 'epoch': 2.83}\n",
      "{'loss': 0.632, 'learning_rate': 8.391629754085842e-06, 'epoch': 2.9}\n",
      "{'loss': 0.6391, 'learning_rate': 8.086146326561785e-06, 'epoch': 2.98}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bea4334929f548e9b00c8fc2cd6170fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/342 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6462925672531128, 'eval_accuracy': 0.6177924217462932, 'eval_runtime': 7.8888, 'eval_samples_per_second': 692.497, 'eval_steps_per_second': 43.352, 'epoch': 3.0}\n",
      "{'loss': 0.6322, 'learning_rate': 7.780662899037727e-06, 'epoch': 3.05}\n",
      "{'loss': 0.6259, 'learning_rate': 7.47517947151367e-06, 'epoch': 3.13}\n",
      "{'loss': 0.627, 'learning_rate': 7.169696043989614e-06, 'epoch': 3.21}\n",
      "{'loss': 0.6294, 'learning_rate': 6.8642126164655576e-06, 'epoch': 3.28}\n",
      "{'loss': 0.6244, 'learning_rate': 6.558729188941501e-06, 'epoch': 3.36}\n",
      "{'loss': 0.6371, 'learning_rate': 6.253245761417443e-06, 'epoch': 3.44}\n",
      "{'loss': 0.6355, 'learning_rate': 5.9477623338933865e-06, 'epoch': 3.51}\n",
      "{'loss': 0.6336, 'learning_rate': 5.64227890636933e-06, 'epoch': 3.59}\n",
      "{'loss': 0.6254, 'learning_rate': 5.336795478845274e-06, 'epoch': 3.67}\n",
      "{'loss': 0.6313, 'learning_rate': 5.031312051321216e-06, 'epoch': 3.74}\n",
      "{'loss': 0.6304, 'learning_rate': 4.7258286237971595e-06, 'epoch': 3.82}\n",
      "{'loss': 0.6294, 'learning_rate': 4.420345196273103e-06, 'epoch': 3.89}\n",
      "{'loss': 0.6366, 'learning_rate': 4.114861768749045e-06, 'epoch': 3.97}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bc7cc1ad2734881a7cca1fe5e97717b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/342 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.648369550704956, 'eval_accuracy': 0.6209042650558302, 'eval_runtime': 7.8795, 'eval_samples_per_second': 693.317, 'eval_steps_per_second': 43.404, 'epoch': 4.0}\n",
      "{'loss': 0.6294, 'learning_rate': 3.809378341224989e-06, 'epoch': 4.05}\n",
      "{'loss': 0.6295, 'learning_rate': 3.503894913700932e-06, 'epoch': 4.12}\n",
      "{'loss': 0.6287, 'learning_rate': 3.198411486176875e-06, 'epoch': 4.2}\n",
      "{'loss': 0.6187, 'learning_rate': 2.8929280586528187e-06, 'epoch': 4.28}\n",
      "{'loss': 0.6265, 'learning_rate': 2.5874446311287615e-06, 'epoch': 4.35}\n",
      "{'loss': 0.6352, 'learning_rate': 2.2819612036047043e-06, 'epoch': 4.43}\n",
      "{'loss': 0.6223, 'learning_rate': 1.9764777760806476e-06, 'epoch': 4.51}\n",
      "{'loss': 0.6219, 'learning_rate': 1.670994348556591e-06, 'epoch': 4.58}\n",
      "{'loss': 0.622, 'learning_rate': 1.3655109210325341e-06, 'epoch': 4.66}\n",
      "{'loss': 0.6202, 'learning_rate': 1.0600274935084774e-06, 'epoch': 4.73}\n",
      "{'loss': 0.6266, 'learning_rate': 7.545440659844204e-07, 'epoch': 4.81}\n",
      "{'loss': 0.6231, 'learning_rate': 4.490606384603636e-07, 'epoch': 4.89}\n",
      "{'loss': 0.6201, 'learning_rate': 1.4357721093630674e-07, 'epoch': 4.96}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f226d244faa4ab2a89aa4af2bf6cc27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/342 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.650716245174408, 'eval_accuracy': 0.6172432729269632, 'eval_runtime': 8.4368, 'eval_samples_per_second': 647.518, 'eval_steps_per_second': 40.537, 'epoch': 5.0}\n",
      "{'train_runtime': 2893.0922, 'train_samples_per_second': 181.023, 'train_steps_per_second': 11.315, 'train_loss': 0.6398471140653937, 'epoch': 5.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=32735, training_loss=0.6398471140653937, metrics={'train_runtime': 2893.0922, 'train_samples_per_second': 181.023, 'train_steps_per_second': 11.315, 'train_loss': 0.6398471140653937, 'epoch': 5.0})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æµ‹è¯•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f13eb45384664b4487827064b6724907",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/342 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.650716245174408,\n",
       " 'eval_accuracy': 0.6172432729269632,\n",
       " 'eval_runtime': 9.1348,\n",
       " 'eval_samples_per_second': 598.046,\n",
       " 'eval_steps_per_second': 37.439}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_validation = AutoModelForSequenceClassification.from_pretrained(\"result/checkpoint-32735\")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model_validation,\n",
    "    args=args,\n",
    "    train_dataset=encoded_data[\"train\"],\n",
    "    eval_dataset=encoded_data[\"validation\"],\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"text-classification\",model=\"result/checkpoint-32735\")\n",
    "\n",
    "predictions = []\n",
    "\n",
    "for item in encoded_data[\"test\"]:\n",
    "    predictions.append(classifier(item[\"question\"]+item[\"sentence\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = \"QNLI.tsv\"\n",
    "\n",
    "with open(output_file, \"w\") as f:\n",
    "    f.write(\"index\\tprediction\\n\")\n",
    "    for i,item in enumerate(predictions):\n",
    "        pred_str = \"not_entailment\" if (item[0][\"label\"] == \"LABEL_0\") else \"entailment\"\n",
    "        f.write(f\"{i}\\t{pred_str}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä¸å®˜æ–¹çš„ç¤ºä¾‹æäº¤ä½œæ¯”è¾ƒ\n",
    "\n",
    "æ³¨æ„ï¼Œå®˜æ–¹çš„ç¤ºä¾‹æäº¤å¹¶æ²¡æœ‰è¯´ä»–æ˜¯ç™¾åˆ†ä¹‹ä¸€ç™¾æ­£ç¡®ç‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å‡†ç¡®ç‡: 0.5058565153733529\n"
     ]
    }
   ],
   "source": [
    "input_file = \"official_QNLI.tsv\"\n",
    "\n",
    "with open(output_file, 'r') as f1:\n",
    "    file1_lines = f1.readlines()\n",
    "\n",
    "with open(input_file, 'r') as f2:\n",
    "    file2_lines = f2.readlines()\n",
    "\n",
    "different_predictions_count = 0\n",
    "total_samples_count = len(file1_lines)  \n",
    "\n",
    "for line1, line2 in zip(file1_lines[1:], file2_lines[1:]):\n",
    "    prediction1 = line1.strip().split('\\t')[1]  \n",
    "    prediction2 = line2.strip().split('\\t')[1]\n",
    "\n",
    "    if prediction1 != prediction2:\n",
    "        different_predictions_count += 1\n",
    "\n",
    "# è®¡ç®—ä¸åŒé¢„æµ‹å€¼çš„æ¯”ä¾‹\n",
    "different_predictions_ratio = different_predictions_count / total_samples_count\n",
    "\n",
    "print(f\"å‡†ç¡®ç‡: {different_predictions_ratio}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "l1word2vec",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
