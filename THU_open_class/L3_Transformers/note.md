# L3_Transformers

## æç¤º

Fine tuningæŒ‡çš„æ˜¯åˆ©ç”¨æ•°æ®é›†å†å¯¹ä½ éœ€è¦çš„ä¸‹æ¸¸ä»»åŠ¡è¿›è¡Œä¸€æ¬¡è®­ç»ƒï¼›

æ¯ä¸ªcheckpointä¿å­˜å¤±è´¥ï¼Œç»§ç»­çœ‹ä¸‹æ–‡ã€‚

## ä»£ç 

æ ¹æ®demoæä¾›çš„æ¨¡æ¿ï¼Œæˆ‘ä»¬åŒæ ·å°†ä»£ç åˆ†ä¸ºï¼š

- æ•°æ®é›†ä¸‹è½½
- tokenization
- æ¨¡å‹ä¸‹è½½
- è®¾å®šå‚æ•°
- è®­ç»ƒ

### æ•°æ®é›†ä¸‹è½½

```python
from datasets import load_dataset, load_metric
dataset = load_dataset("glue", "qnli")
metric = load_metric("glue", "qnli")
```

### Tokenization

```python
from transformers import AutoTokenizer, AutoModelForSequenceClassification
tokenizer = AutoTokenizer.from_pretrained("google/bert_uncased_L-2_H-128_A-2")
def preprocess_function(examples):
    return tokenizer(examples['sentence'], truncation=True, max_length=512)
encoded_data = dataset.map(preprocess_function,batched=True)
```

### æ¨¡å‹ä¸‹è½½

```python
model = AutoModelForSequenceClassification.from_pretrained("google/bert_uncased_L-2_H-128_A-2",num_labels=2)
```

### è®¾å®šå‚æ•°

```python
from transformers import TrainingArguments

batch_size = 16

args = TrainingArguments(
    output_dir="./result",
    evaluation_strategy="epoch", # åœ¨æ¯ä¸ªepochç»“æŸåæµ‹è¯•æ•ˆæœ
    save_strategy="epoch", # æ¯ä¸ªepochç»“æŸåä¿å­˜æ¨¡å‹
    learning_rate=2e-5, # å­¦ä¹ ç‡
    per_device_train_batch_size=batch_size, # æ¯ä¸ªGPUè®­ç»ƒçš„batch size
    per_device_eval_batch_size=batch_size, # æ¯ä¸ªGPUæµ‹è¯•çš„batch size
    num_train_epochs=5, # è®­ç»ƒçš„epochæ•°
    weight_decay=0.01, # æƒé‡è¡°å‡
    load_best_model_at_end=True, # è®­ç»ƒç»“æŸåï¼Œæ˜¯å¦åŠ è½½åœ¨éªŒè¯é›†ä¸Šè¡¨ç°æœ€å¥½çš„æ¨¡å‹
    metric_for_best_model="accuracy" # å‡†ç¡®ç‡ä¸ºæŒ‡æ ‡
)
```

### è®­ç»ƒ

```python
from transformers import Trainer
import numpy as np
def compute_metrics(eval_pred):
    logits, lables = eval_pred
    predictions = np.argmax(logits,axis=1)
    return metric.compute(predictions=predictions, references=lables)
trainer = Trainer(
    model=model,
    args=args,
    train_dataset=encoded_data["train"],
    eval_dataset=encoded_data["validation"],
    compute_metrics=compute_metrics,
    tokenizer=tokenizer
)
trainer.train()
```

### ä¿å­˜å¤±è´¥

è¿™ä¸ªé—®é¢˜ä¹Ÿæ˜¯å›°æ‰°äº†æˆ‘ä¸€ä¸¤ä¸ªå°æ—¶ï¼Œç„¶åæˆ‘åœ¨huggingface forumä¸Šæ‰¾åˆ°ä¸€ä¸ª[[Permission issues when saving model checkpoint - Beginners - Hugging Face Forums](https://discuss.huggingface.co/t/permission-issues-when-saving-model-checkpoint/70109/2)]ï¼Œä½†æ˜¯ä¸å¤ªæ¸…æ¥šåŸç†ï¼Œä½†æ˜¯æœ‰ç”¨ã€‚

## ç»“æœ

è¿™é‡Œæˆ‘çš„ç¯å¢ƒåªæœ‰CPUï¼Œè·‘çš„æ—¶é—´éå¸¸é•¿ğŸ‘¼ï¼›

```
{'eval_loss': 0.650716245174408, 'eval_accuracy': 0.6172432729269632, 'eval_runtime': 8.4368, 'eval_samples_per_second': 647.518, 'eval_steps_per_second': 40.537, 'epoch': 5.0}
```

```
TrainOutput(global_step=32735, training_loss=0.6398471140653937, metrics={'train_runtime': 2893.0922, 'train_samples_per_second': 181.023, 'train_steps_per_second': 11.315, 'train_loss': 0.6398471140653937, 'epoch': 5.0})
```

é€šè¿‡trainer.evaluate()è¾“å‡ºçš„ç»“æœï¼›

```
{'eval_loss': 0.650716245174408,
 'eval_accuracy': 0.6172432729269632,
 'eval_runtime': 9.1348,
 'eval_samples_per_second': 598.046,
 'eval_steps_per_second': 37.439}
```

é™„åŠ ï¼Œé€šè¿‡pipelineï¼Œæˆ‘ä»¬å¯ä»¥å°è¯•é¢„æµ‹testé›†ä¸Šçš„labelï¼›

```python
from transformers import pipeline

# Load the pre-trained model
classifier = pipeline("text-classification",model="result/checkpoint-32735")

# predictions = classifier(encoded_data["test"][0]["question"]+encoded_data["test"][0]["sentence"])

predictions = []

for item in encoded_data["test"]:
    predictions.append(classifier(item["question"]+item["sentence"]))
    
output_file = "QNLI.tsv"

with open(output_file, "w") as f:
    f.write("index\tprediction\n")
    for i,item in enumerate(predictions):
        pred_str = "not_entailment" if (item[0]["label"] == "LABEL_0") else "entailment"
        f.write(f"{i}\t{pred_str}\n")

```

ä»£ç ä¸‹é¢æˆ‘è¿˜ä»å®˜æ–¹ä¸‹è½½äº†QNLIçš„ç¤ºä¾‹æäº¤æ•°æ®ï¼Œä½†æ˜¯å®˜æ–¹å¹¶æ²¡æœ‰è¯´æ˜è¿™ä¸€ä»½æäº¤çš„æ ‡ç­¾æ˜¯å¦å®Œå…¨ä¸ç­”æ¡ˆä¸€è‡´ï¼Œæ‰€ä»¥ä»…ä¾›å‚è€ƒã€‚
